{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qlulYX9lue9y"
      },
      "outputs": [],
      "source": [
        "#Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
        "#Ans:-In machine learning, particularly in the context of Support Vector Machines (SVMs) and other kernelized algorithms,\n",
        "# there is a close relationship between polynomial functions and kernel functions.\n",
        "# Let's explore this relationship:\n",
        "#Kernel Functions:\n",
        "#Kernel functions play a crucial role in kernelized algorithms, including SVMs.\n",
        "#In SVMs, the kernel function computes the dot product of the transformed input data in a higher-dimensional space without explicitly computing the transformation.\n",
        "#The choice of the kernel function determines the shape of the decision boundary and the ability of the SVM to handle complex relationships in the data.\n",
        "\n",
        "#Polynomial Kernel:\n",
        "#The polynomial kernel is a specific type of kernel function used in SVMs.\n",
        "#It is defined as K(x,y) = (x⋅y + c)^d , where d is the degree of the polynomial, and c is a constant.\n",
        "#The polynomial kernel implicitly maps the input data into a higher-dimensional space, allowing the SVM to capture nonlinear relationships.\n",
        "\n",
        "#Relationship:\n",
        "#Polynomial kernels are a type of kernel function, and they provide a way to introduce nonlinearity into SVMs.\n",
        "#The polynomial kernel essentially computes the dot product in a higher-dimensional space, making it possible to model complex decision boundaries.\n",
        "#The degree of the polynomial (d) controls the complexity of the decision boundary. Higher degrees allow the model to capture more intricate patterns but may also increase the risk of overfitting.\n",
        "\n",
        "#Other Kernel Functions:\n",
        "#While polynomial kernels are one type of kernel function, there are other types such as linear kernels, radial basis function (RBF) kernels, and more.\n",
        "#Linear kernels are equivalent to using a polynomial kernel with degree 1.\n",
        "#RBF kernels, also known as Gaussian kernels, provide a smooth and flexible way to capture complex relationships."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
        "#Ans:-\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the SVM model with a polynomial kernel\n",
        "svm_model = SVC(kernel='poly', degree=3)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhWNQEbiwK7m",
        "outputId": "41e4eb62-6739-4d7b-c78e-11180ee5ea1d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
        "#Ans:-In Support Vector Regression (SVR), the epsilon parameter is a crucial hyperparameter that defines the width of the epsilon-insensitive tube.\n",
        "# The epsilon-insensitive tube is a region around the predicted values within which errors are ignored. Outside this tube, errors are penalized.\n",
        "# The width of the tube is controlled by the epsilon parameter.\n",
        "#Now, let's discuss how increasing the value of epsilon affects the number of support vectors in SVR:\n",
        "#Smaller Epsilon:\n",
        "#When epsilon is small, the insensitive tube is narrow.\n",
        "#SVR aims to fit the training data within this narrow tube, allowing for less tolerance for errors.\n",
        "#As a result, SVR may need to use more support vectors to fit the data accurately within the narrow tube.\n",
        "\n",
        "#Larger Epsilon:\n",
        "#When epsilon is large, the insensitive tube is wider.\n",
        "#SVR becomes more tolerant of errors as it allows a larger margin for deviations from the predicted values.\n",
        "#With a wider tube, SVR may use fewer support vectors to fit the data, as it allows more flexibility for points to fall outside the tube without significant penalty.\n",
        "\n",
        "#Effect on Number of Support Vectors:\n",
        "#Increasing the value of epsilon generally leads to a reduction in the number of support vectors.\n",
        "#A larger epsilon allows for a looser fit, and SVR does not need to include as many data points as support vectors to satisfy the model constraints.\n",
        "#The number of support vectors tends to decrease as epsilon increases because the model becomes more tolerant of errors, and the tube within which errors are ignored becomes wider.\n",
        "\n",
        "#Balance:\n",
        "#The choice of epsilon involves a trade-off. A smaller epsilon can lead to a more accurate fit but may also result in overfitting, especially if the data has noise.\n",
        "#A larger epsilon provides a more flexible and generalized fit, potentially avoiding overfitting, but it may sacrifice some accuracy on the training data."
      ],
      "metadata": {
        "id": "TTsNcqcAwhge"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)?\n",
        "# Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?\n",
        "#Ans:-Support Vector Regression (SVR) is influenced by several hyperparameters that significantly impact its performance.\n",
        "# Let's discuss the key hyperparameters—kernel function, C parameter, epsilon parameter (ϵ), and gamma parameter (γ)—and how adjusting their values can affect the performance of SVR:\n",
        "#Kernel Function:The kernel function determines the type of transformation applied to the input data.\n",
        "#Common kernel functions include linear, polynomial, radial basis function (RBF or Gaussian), and sigmoid.\n",
        "#Example:Use a linear kernel (kernel='linear') when the relationship between features and the target is expected to be approximately linear.\n",
        "#Use an RBF kernel (kernel='rbf') for non-linear relationships.\n",
        "\n",
        "#C Parameter:The C parameter controls the trade-off between achieving a smooth fit and fitting the training data accurately.\n",
        "#A smaller C allows for a smoother fit with more tolerance for errors, while a larger C enforces a more accurate fit with less tolerance.\n",
        "#Example:If the data has noise or outliers, a smaller C may be preferred to avoid overfitting.\n",
        "#If a more accurate fit is desired, especially when the data is not noisy, a larger C may be suitable.\n",
        "\n",
        "#Epsilon Parameter (ϵ):The epsilon parameter defines the width of the epsilon-insensitive tube. It specifies the tolerance for errors within this tube.\n",
        "#A smaller epsilon makes the tube narrower, and SVR aims to fit the data within this narrow tube.\n",
        "#A larger epsilon allows for a wider tube, providing more tolerance for errors.\n",
        "#Example:Use a smaller epsilon if you want to enforce a more precise fit with less tolerance for deviations.\n",
        "#Use a larger epsilon if you want to allow more flexibility and are willing to tolerate larger errors.\n",
        "\n",
        "#Gamma Parameter (γ):\n",
        "#The gamma parameter influences the shape of the decision boundary and the flexibility of the model.\n",
        "#For the RBF kernel, gamma defines the size of the radial basis function's influence. Higher gamma values result in a more complex decision boundary.\n",
        "#Example:A smaller gamma may be suitable for relatively simple relationships in the data, preventing overfitting.\n",
        "#A larger gamma can capture intricate patterns in the data, but it may lead to overfitting if not carefully tuned."
      ],
      "metadata": {
        "id": "M-LtiDpkxOtl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q5. Assignment:\n",
        "#Import the necessary libraries and load the dataseg\n",
        "# Split the dataset into training and testing setZ\n",
        "# Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
        "# Create an instance of the SVC classifier and train it on the training datW\n",
        "# hse the trained classifier to predict the labels of the testing datW\n",
        "# Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy, precision, recall, F1-scoreK\n",
        "# Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
        "#improve its performanc_\n",
        "# Train the tuned classifier on the entire dataseg\n",
        "# Save the trained classifier to a file for future use.\n",
        "#Ans:-\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import joblib\n",
        "\n",
        "# Load the dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocess the data (scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create an instance of the SVC classifier and train it on the training data\n",
        "svc = SVC()\n",
        "svc.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Use the trained classifier to predict the labels of the testing data\n",
        "y_pred = svc.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the performance of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\\n\", classification_rep)\n",
        "\n",
        "# Tune the hyperparameters using GridSearchCV\n",
        "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly'], 'degree': [2, 3, 4]}\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best parameters from the grid search\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Train the tuned classifier on the entire dataset\n",
        "tuned_svc = grid_search.best_estimator_\n",
        "tuned_svc.fit(X, y)\n",
        "\n",
        "# Save the trained classifier to a file for future use\n",
        "joblib.dump(tuned_svc, 'tuned_svc_model.joblib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zlu1LUJ5yBhr",
        "outputId": "89bb3d59-7093-4b0a-8e04-7b039a936cb8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Best Hyperparameters: {'C': 10, 'degree': 2, 'kernel': 'linear'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tuned_svc_model.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Ey6QTEJyqYt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}